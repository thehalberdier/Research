[
  {
    "objectID": "rust_primer.html",
    "href": "rust_primer.html",
    "title": "Rust Primer for C++/Java Developers",
    "section": "",
    "text": "Goal: Get you to ~80% of Rust, fast. Assumes you know C++/Java, understand types, pointers, and memory. Skips: async runtime internals, FFI, advanced lifetime gymnastics."
  },
  {
    "objectID": "rust_primer.html#first-things-first-cargo-project-setup",
    "href": "rust_primer.html#first-things-first-cargo-project-setup",
    "title": "Rust Primer for C++/Java Developers",
    "section": "1. First Things First: Cargo & Project Setup",
    "text": "1. First Things First: Cargo & Project Setup\nCargo is Rust‚Äôs build system + package manager. Think CMake + Conan + Maven rolled into one, but actually pleasant.\ncargo new myproject        # creates a new binary project\ncargo new mylib --lib      # creates a library\ncargo build                # compile (debug)\ncargo build --release      # compile (optimized)\ncargo run                  # build + run\ncargo test                 # run tests\ncargo add serde            # add a dependency (like `npm install`)\nProject structure:\nmyproject/\n‚îú‚îÄ‚îÄ Cargo.toml             # like pom.xml / CMakeLists.txt\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ main.rs            # entry point (fn main)\nCargo.toml declares dependencies:\n[dependencies]\nserde = { version = \"1.0\", features = [\"derive\"] }\ntokio = { version = \"1\", features = [\"full\"] }\nCrates.io is the package registry (like Maven Central / crates ‚âà packages)."
  },
  {
    "objectID": "rust_primer.html#the-big-idea-ownership",
    "href": "rust_primer.html#the-big-idea-ownership",
    "title": "Rust Primer for C++/Java Developers",
    "section": "2. The Big Idea: Ownership",
    "text": "2. The Big Idea: Ownership\nThis is THE thing that makes Rust different. If you internalize this section, the rest falls into place.\n\nThe Three Rules\n\nEvery value has exactly one owner (a variable).\nWhen the owner goes out of scope, the value is dropped (destructor runs, memory freed).\nOwnership can be moved, but then the old variable is invalid.\n\nfn main() {\n    let s1 = String::from(\"hello\");  // s1 owns the String\n    let s2 = s1;                      // ownership MOVED to s2\n    // println!(\"{}\", s1);            // COMPILE ERROR: s1 is no longer valid\n    println!(\"{}\", s2);               // fine\n}\nC++ analogy: Think std::unique_ptr. Assignment = move. But Rust enforces this for ALL types at compile time, not just smart pointers.\nJava comparison: In Java, s2 = s1 makes both point to the same object. In Rust, s1 is dead after the move.\n\n\nStack vs.¬†Heap & Copy Types\nSimple scalar types (i32, f64, bool, char, tuples of these) implement Copy ‚Äî they‚Äôre copied on assignment, not moved. Just like in C++.\nlet x = 5;\nlet y = x;       // x is copied, both valid\nprintln!(\"{x} {y}\"); // fine\nHeap-allocated types (String, Vec&lt;T&gt;, Box&lt;T&gt;) are moved.\n\n\nClone: Explicit Deep Copy\nIf you actually want a deep copy, say so:\nlet s1 = String::from(\"hello\");\nlet s2 = s1.clone();  // explicit deep copy\nprintln!(\"{s1} {s2}\"); // both valid"
  },
  {
    "objectID": "rust_primer.html#borrowing-references",
    "href": "rust_primer.html#borrowing-references",
    "title": "Rust Primer for C++/Java Developers",
    "section": "3. Borrowing & References",
    "text": "3. Borrowing & References\nYou don‚Äôt always want to move ownership. Borrowing lets you lend access.\nfn print_length(s: &String) {   // borrows s (immutable reference)\n    println!(\"len = {}\", s.len());\n}\n\nfn main() {\n    let s = String::from(\"hello\");\n    print_length(&s);            // lend s\n    println!(\"{s}\");             // still valid ‚Äî we only lent it\n}\n\nThe Borrowing Rules\nAt any given time, you can have either: - Any number of immutable references (&T), OR - Exactly one mutable reference (&mut T)\nNever both at the same time.\nlet mut s = String::from(\"hello\");\n\nlet r1 = &s;       // ok\nlet r2 = &s;       // ok ‚Äî multiple immutable refs fine\n// let r3 = &mut s; // COMPILE ERROR: can't borrow mutably while immutable refs exist\n\nprintln!(\"{r1} {r2}\");\n// r1, r2 no longer used after this point (their lifetime ends)\n\nlet r3 = &mut s;   // now ok ‚Äî no immutable refs alive\nr3.push_str(\" world\");\nWhy: This is Rust‚Äôs compile-time data race prevention. No reader-writer conflicts, ever.\nC++ analogy: Like const T& vs T&, but the compiler actually enforces that you can‚Äôt alias a mutable ref."
  },
  {
    "objectID": "rust_primer.html#lifetimes-the-minimum-you-need",
    "href": "rust_primer.html#lifetimes-the-minimum-you-need",
    "title": "Rust Primer for C++/Java Developers",
    "section": "4. Lifetimes (The Minimum You Need)",
    "text": "4. Lifetimes (The Minimum You Need)\nLifetimes ensure references don‚Äôt outlive the data they point to. Most of the time, the compiler infers them. You only write them when the compiler can‚Äôt figure it out ‚Äî typically in function signatures returning references.\n// \"The returned reference lives as long as BOTH inputs live\"\nfn longer&lt;'a&gt;(s1: &'a str, s2: &'a str) -&gt; &'a str {\n    if s1.len() &gt;= s2.len() { s1 } else { s2 }\n}\n'a is a lifetime parameter. Read it as: ‚Äúthe output reference is valid for at least as long as both input references.‚Äù\nPractical rule of thumb: If the compiler says ‚Äúmissing lifetime specifier,‚Äù add &lt;'a&gt; to the function and annotate the references. If you‚Äôre storing references in a struct, you need lifetimes on the struct too:\nstruct Excerpt&lt;'a&gt; {\n    text: &'a str,   // this struct can't outlive the string it borrows\n}\nEscape hatch: If lifetimes get painful, just own the data instead (String instead of &str, Vec&lt;T&gt; instead of &[T]). Cloning is fine. Don‚Äôt fight the borrow checker when you‚Äôre learning."
  },
  {
    "objectID": "rust_primer.html#types-structs-enums",
    "href": "rust_primer.html#types-structs-enums",
    "title": "Rust Primer for C++/Java Developers",
    "section": "5. Types, Structs, Enums",
    "text": "5. Types, Structs, Enums\n\nPrimitive Types\nlet x: i32 = 42;        // signed: i8, i16, i32, i64, i128, isize\nlet y: u64 = 100;       // unsigned: u8, u16, u32, u64, u128, usize\nlet f: f64 = 3.14;      // f32, f64\nlet b: bool = true;\nlet c: char = 'ü¶Ä';     // 4 bytes, Unicode scalar value\nusize / isize = pointer-sized integers. Used for indexing.\n\n\nStructs (like C++ structs / Java classes, but no inheritance)\nstruct Server {\n    hostname: String,\n    cpu_cores: u32,\n    is_active: bool,\n}\n\nimpl Server {\n    // Associated function (like static method). Convention: `new` is the constructor.\n    fn new(hostname: String, cores: u32) -&gt; Self {\n        Server {\n            hostname,          // shorthand when field name == variable name\n            cpu_cores: cores,\n            is_active: true,\n        }\n    }\n\n    // Method: takes &self (immutable borrow of the instance)\n    fn summary(&self) -&gt; String {\n        format!(\"{}: {} cores\", self.hostname, self.cpu_cores)\n    }\n\n    // Mutable method: takes &mut self\n    fn deactivate(&mut self) {\n        self.is_active = false;\n    }\n}\n\nfn main() {\n    let mut srv = Server::new(\"node-01\".to_string(), 64);\n    println!(\"{}\", srv.summary());\n    srv.deactivate();\n}\nNo inheritance. Composition + traits instead (see ¬ß7).\n\n\nEnums (WAY more powerful than C++/Java enums)\nRust enums are algebraic data types (tagged unions). Each variant can hold data.\nenum StorageBackend {\n    Local { path: String },\n    Ceph { pool: String, namespace: String },\n    NVMe { device_id: u32 },\n    None,\n}\n\nfn describe(backend: &StorageBackend) {\n    match backend {\n        StorageBackend::Local { path } =&gt; println!(\"Local at {path}\"),\n        StorageBackend::Ceph { pool, .. } =&gt; println!(\"Ceph pool: {pool}\"),\n        StorageBackend::NVMe { device_id } =&gt; println!(\"NVMe dev {device_id}\"),\n        StorageBackend::None =&gt; println!(\"No storage\"),\n    }\n}\nC++ analogy: std::variant, but with exhaustive pattern matching enforced by the compiler."
  },
  {
    "objectID": "rust_primer.html#pattern-matching",
    "href": "rust_primer.html#pattern-matching",
    "title": "Rust Primer for C++/Java Developers",
    "section": "6. Pattern Matching",
    "text": "6. Pattern Matching\nmatch is like switch on steroids. It‚Äôs exhaustive ‚Äî you must handle every case.\nlet x = 42;\n\nmatch x {\n    0 =&gt; println!(\"zero\"),\n    1..=10 =&gt; println!(\"small\"),\n    11 | 12 =&gt; println!(\"eleven or twelve\"),\n    n if n &lt; 0 =&gt; println!(\"negative: {n}\"),   // guard\n    _ =&gt; println!(\"something else\"),             // _ = default\n}\n\nif let ‚Äî when you only care about one variant\nlet maybe_name: Option&lt;String&gt; = Some(\"venkat\".to_string());\n\nif let Some(name) = maybe_name {\n    println!(\"Got: {name}\");\n}\n// Use this instead of a full match when you don't care about the other cases.\n\n\nlet ... else ‚Äî early return on mismatch (Rust 1.65+)\nfn process(val: Option&lt;i32&gt;) {\n    let Some(x) = val else {\n        println!(\"was None, bailing\");\n        return;\n    };\n    println!(\"processing {x}\");\n}"
  },
  {
    "objectID": "rust_primer.html#traits-interfaces-typeclasses",
    "href": "rust_primer.html#traits-interfaces-typeclasses",
    "title": "Rust Primer for C++/Java Developers",
    "section": "7. Traits (‚âà Interfaces + Typeclasses)",
    "text": "7. Traits (‚âà Interfaces + Typeclasses)\nTraits define shared behavior. Like Java interfaces, but can have default implementations and can be implemented for types you didn‚Äôt define.\ntrait Describable {\n    fn describe(&self) -&gt; String;\n\n    // Default implementation (like Java default methods)\n    fn short_desc(&self) -&gt; String {\n        format!(\"{}...\", &self.describe()[..20.min(self.describe().len())])\n    }\n}\n\nimpl Describable for Server {\n    fn describe(&self) -&gt; String {\n        format!(\"Server {} with {} cores\", self.hostname, self.cpu_cores)\n    }\n}\n\nTrait Bounds (= Generics constrained by traits)\n// Any T that implements Describable\nfn log_item&lt;T: Describable&gt;(item: &T) {\n    println!(\"[LOG] {}\", item.describe());\n}\n\n// Equivalent with `where` clause (cleaner for multiple bounds)\nfn log_item2&lt;T&gt;(item: &T)\nwhere\n    T: Describable + Clone + std::fmt::Debug,\n{\n    println!(\"[LOG] {:?} ‚Äî {}\", item, item.describe());\n}\n\n\nimpl Trait syntax (simpler)\nfn log_item3(item: &impl Describable) {  // same as the generic version above\n    println!(\"[LOG] {}\", item.describe());\n}\n\nfn make_server() -&gt; impl Describable {    // return \"some type that is Describable\"\n    Server::new(\"anon\".into(), 8)\n}\n\n\nCommon Standard Library Traits You‚Äôll Use Constantly\n\n\n\nTrait\nWhat it does\nC++/Java analogy\n\n\n\n\nClone\nExplicit deep copy (.clone())\nCopy constructor\n\n\nCopy\nImplicit bitwise copy\nPOD types\n\n\nDebug\nFormat with {:?} for debugging\ntoString() for debug\n\n\nDisplay\nFormat with {} for user-facing output\ntoString()\n\n\nPartialEq, Eq\n== / !=\noperator== / equals()\n\n\nPartialOrd, Ord\n&lt;, &gt;, sorting\noperator&lt; / Comparable\n\n\nHash\nHashing for HashMap keys\nhashCode()\n\n\nDefault\nDefault value (T::default())\nDefault constructor\n\n\nFrom / Into\nType conversion\nConversion constructors\n\n\nIterator\nIteration protocol\nIterator&lt;T&gt;\n\n\nDrop\nDestructor\nDestructor / finalize()\n\n\n\nMost of these can be auto-derived:\n#[derive(Debug, Clone, PartialEq, Eq, Hash)]\nstruct NodeId {\n    rack: u32,\n    slot: u32,\n}"
  },
  {
    "objectID": "rust_primer.html#error-handling-result-and-option",
    "href": "rust_primer.html#error-handling-result-and-option",
    "title": "Rust Primer for C++/Java Developers",
    "section": "8. Error Handling: Result and Option",
    "text": "8. Error Handling: Result and Option\nNo exceptions in Rust. Errors are values.\n\nOption&lt;T&gt; ‚Äî value might be absent\n// Option is just: enum Option&lt;T&gt; { Some(T), None }\n\nfn find_server(id: u32) -&gt; Option&lt;Server&gt; {\n    if id == 1 {\n        Some(Server::new(\"node-01\".into(), 64))\n    } else {\n        None\n    }\n}\n\n// Handling:\nmatch find_server(1) {\n    Some(srv) =&gt; println!(\"{}\", srv.summary()),\n    None =&gt; println!(\"not found\"),\n}\n\n// Or more concisely:\nlet srv = find_server(1).unwrap();             // panics if None (like .get() on null)\nlet srv = find_server(1).unwrap_or(default);   // fallback\nlet srv = find_server(1).expect(\"server 1 must exist\"); // panic with message\n\n\nResult&lt;T, E&gt; ‚Äî operation might fail\n// Result is just: enum Result&lt;T, E&gt; { Ok(T), Err(E) }\n\nuse std::fs;\nuse std::io;\n\nfn read_config(path: &str) -&gt; Result&lt;String, io::Error&gt; {\n    fs::read_to_string(path)\n}\n\nfn main() {\n    match read_config(\"/etc/myapp.conf\") {\n        Ok(contents) =&gt; println!(\"Config: {contents}\"),\n        Err(e) =&gt; eprintln!(\"Failed: {e}\"),\n    }\n}\n\n\nThe ? Operator (This Is Huge)\n? propagates errors upward. If the Result is Err, return early with that error. If Ok, unwrap the value. This replaces try-catch with zero boilerplate.\nfn load_and_parse_config(path: &str) -&gt; Result&lt;Config, Box&lt;dyn std::error::Error&gt;&gt; {\n    let contents = fs::read_to_string(path)?;   // returns Err early if file read fails\n    let config: Config = serde_json::from_str(&contents)?;  // returns Err if parse fails\n    Ok(config)\n}\nC++ analogy: Like if every function returned std::expected&lt;T, E&gt; and you had a concise syntax for early returns.\nPractical advice: Use anyhow::Result (from the anyhow crate) for applications and thiserror for libraries.\n// In Cargo.toml: anyhow = \"1\"\nuse anyhow::{Context, Result};\n\nfn load_config(path: &str) -&gt; Result&lt;Config&gt; {\n    let contents = fs::read_to_string(path)\n        .context(\"failed to read config file\")?;    // adds context to error\n    let config = serde_json::from_str(&contents)\n        .context(\"failed to parse config\")?;\n    Ok(config)\n}"
  },
  {
    "objectID": "rust_primer.html#collections-iterators",
    "href": "rust_primer.html#collections-iterators",
    "title": "Rust Primer for C++/Java Developers",
    "section": "9. Collections & Iterators",
    "text": "9. Collections & Iterators\n\nKey Collections\nuse std::collections::HashMap;\n\n// Vec&lt;T&gt; ‚Äî dynamic array (like std::vector / ArrayList)\nlet mut nums: Vec&lt;i32&gt; = vec![1, 2, 3];\nnums.push(4);\n\n// String ‚Äî owned, growable UTF-8 string\nlet mut s = String::from(\"hello\");\ns.push_str(\" world\");\n\n// &str ‚Äî string slice (borrowed view into a String or literal)\nlet slice: &str = &s[0..5];\n\n// HashMap&lt;K, V&gt; ‚Äî hash map (like std::unordered_map / HashMap)\nlet mut scores: HashMap&lt;String, i32&gt; = HashMap::new();\nscores.insert(\"node-01\".into(), 95);\n\n// VecDeque, BTreeMap, BTreeSet, HashSet also available\n\n\nIterators (Rust‚Äôs Secret Weapon)\nRust iterators are lazy, zero-cost abstractions. They compile down to the same code as hand-written loops.\nlet nums = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n\n// Chained iterator operations\nlet result: Vec&lt;i32&gt; = nums.iter()\n    .filter(|&&x| x % 2 == 0)       // keep evens\n    .map(|&x| x * x)                // square them\n    .collect();                       // collect into Vec\n// result = [4, 16, 36, 64, 100]\n\n// Sum\nlet total: i32 = nums.iter().sum();\n\n// Find\nlet first_big = nums.iter().find(|&&x| x &gt; 5);  // Option&lt;&i32&gt;\n\n// for loop (uses IntoIterator under the hood)\nfor num in &nums {\n    println!(\"{num}\");\n}\n\n// Enumerate (like Python)\nfor (i, num) in nums.iter().enumerate() {\n    println!(\"[{i}] = {num}\");\n}\n\n// Iterating over HashMap\nfor (key, value) in &scores {\n    println!(\"{key}: {value}\");\n}\nKey iterator methods: map, filter, flat_map, fold, reduce, take, skip, zip, enumerate, any, all, find, position, collect, count, sum, min, max, chain, peekable.\n.collect() can produce different collection types based on the type annotation ‚Äî it‚Äôs generic over the output type."
  },
  {
    "objectID": "rust_primer.html#closures",
    "href": "rust_primer.html#closures",
    "title": "Rust Primer for C++/Java Developers",
    "section": "10. Closures",
    "text": "10. Closures\n// Type-inferred closure\nlet add = |a, b| a + b;\nprintln!(\"{}\", add(2, 3));\n\n// With explicit types\nlet square = |x: i32| -&gt; i32 { x * x };\n\n// Closures capture variables from their environment\nlet threshold = 50;\nlet above_threshold = |x: &i32| *x &gt; threshold;    // borrows `threshold`\n\nlet results: Vec&lt;&i32&gt; = nums.iter().filter(|x| above_threshold(x)).collect();\n\n// Move closure ‚Äî takes ownership of captured variables\nlet name = String::from(\"venkat\");\nlet greeting = move || println!(\"Hello, {name}\");    // `name` moved into closure\n// `name` is no longer usable here\ngreeting();\nClosures implement one or more of: Fn (borrows immutably), FnMut (borrows mutably), FnOnce (takes ownership). The compiler figures this out."
  },
  {
    "objectID": "rust_primer.html#smart-pointers",
    "href": "rust_primer.html#smart-pointers",
    "title": "Rust Primer for C++/Java Developers",
    "section": "11. Smart Pointers",
    "text": "11. Smart Pointers\n\n\n\n\n\n\n\n\nType\nWhat it does\nC++ analogy\n\n\n\n\nBox&lt;T&gt;\nHeap allocation, single owner\nstd::unique_ptr&lt;T&gt;\n\n\nRc&lt;T&gt;\nReference-counted, single thread\nstd::shared_ptr&lt;T&gt;\n\n\nArc&lt;T&gt;\nReference-counted, thread-safe\nstd::shared_ptr&lt;T&gt; (atomic)\n\n\nRefCell&lt;T&gt;\nInterior mutability (runtime borrow checks)\n‚Äî\n\n\nMutex&lt;T&gt;\nMutual exclusion with owned data\nstd::mutex + the data it protects\n\n\nRwLock&lt;T&gt;\nReader-writer lock with owned data\nstd::shared_mutex + data\n\n\n\n// Box: heap allocation\nlet boxed: Box&lt;i32&gt; = Box::new(42);\n\n// Arc + Mutex: shared mutable state across threads\nuse std::sync::{Arc, Mutex};\n\nlet counter = Arc::new(Mutex::new(0));\nlet counter_clone = Arc::clone(&counter);\n\nstd::thread::spawn(move || {\n    let mut num = counter_clone.lock().unwrap();\n    *num += 1;\n});\nKey insight: In Rust, Mutex&lt;T&gt; owns the data it protects. You can‚Äôt access the data without locking. This makes data races structurally impossible."
  },
  {
    "objectID": "rust_primer.html#concurrency",
    "href": "rust_primer.html#concurrency",
    "title": "Rust Primer for C++/Java Developers",
    "section": "12. Concurrency",
    "text": "12. Concurrency\n\nThreads\nuse std::thread;\n\nlet handle = thread::spawn(|| {\n    println!(\"hello from thread\");\n    42   // return value\n});\n\nlet result = handle.join().unwrap();  // wait + get result\n\n\nChannels (message passing)\nuse std::sync::mpsc;  // multi-producer, single-consumer\n\nlet (tx, rx) = mpsc::channel();\nlet tx2 = tx.clone();   // clone sender for multiple producers\n\nthread::spawn(move || { tx.send(\"from thread 1\").unwrap(); });\nthread::spawn(move || { tx2.send(\"from thread 2\").unwrap(); });\n\nfor msg in rx {  // blocks until all senders dropped\n    println!(\"Got: {msg}\");\n}\n\n\nAsync/Await (brief overview)\nRust has async syntax but no built-in runtime. You pick one (usually tokio).\n// Cargo.toml: tokio = { version = \"1\", features = [\"full\"] }\n\n#[tokio::main]\nasync fn main() {\n    let result = fetch_data().await;\n    println!(\"{result}\");\n}\n\nasync fn fetch_data() -&gt; String {\n    // async operations here\n    \"data\".to_string()\n}"
  },
  {
    "objectID": "rust_primer.html#modules-visibility",
    "href": "rust_primer.html#modules-visibility",
    "title": "Rust Primer for C++/Java Developers",
    "section": "13. Modules & Visibility",
    "text": "13. Modules & Visibility\n// In src/main.rs or src/lib.rs\n\nmod network {                    // inline module\n    pub struct Connection {      // pub = public\n        pub host: String,\n        port: u16,               // private by default\n    }\n\n    impl Connection {\n        pub fn new(host: String, port: u16) -&gt; Self {\n            Connection { host, port }\n        }\n\n        pub fn connect(&self) {\n            println!(\"Connecting to {}:{}\", self.host, self.port);\n        }\n    }\n\n    pub mod dns {                // nested module\n        pub fn resolve(hostname: &str) -&gt; String {\n            format!(\"192.168.1.1 (resolved {hostname})\")\n        }\n    }\n}\n\nfn main() {\n    let conn = network::Connection::new(\"10.0.0.1\".into(), 8080);\n    conn.connect();\n    let ip = network::dns::resolve(\"node-01\");\n}\nFor file-based modules:\nsrc/\n‚îú‚îÄ‚îÄ main.rs         // declares: mod network;\n‚îú‚îÄ‚îÄ network/\n‚îÇ   ‚îú‚îÄ‚îÄ mod.rs      // declares: pub mod dns;\n‚îÇ   ‚îî‚îÄ‚îÄ dns.rs      // the dns module\nOr the newer flat style (Rust 2018+):\nsrc/\n‚îú‚îÄ‚îÄ main.rs         // declares: mod network;\n‚îú‚îÄ‚îÄ network.rs      // the network module, declares: pub mod dns;\n‚îú‚îÄ‚îÄ network/\n‚îÇ   ‚îî‚îÄ‚îÄ dns.rs\nuse brings items into scope:\nuse network::Connection;\nuse std::collections::HashMap;\nuse std::io::{self, Read, Write};  // multiple imports"
  },
  {
    "objectID": "rust_primer.html#generics",
    "href": "rust_primer.html#generics",
    "title": "Rust Primer for C++/Java Developers",
    "section": "14. Generics",
    "text": "14. Generics\nLike C++ templates / Java generics, but with trait bounds instead of SFINAE / concepts.\n// Generic function\nfn largest&lt;T: PartialOrd&gt;(list: &[T]) -&gt; &T {\n    let mut largest = &list[0];\n    for item in &list[1..] {\n        if item &gt; largest {\n            largest = item;\n        }\n    }\n    largest\n}\n\n// Generic struct\nstruct Cache&lt;K, V&gt; {\n    entries: HashMap&lt;K, V&gt;,\n    max_size: usize,\n}\n\nimpl&lt;K: Eq + Hash, V&gt; Cache&lt;K, V&gt; {\n    fn new(max_size: usize) -&gt; Self {\n        Cache {\n            entries: HashMap::new(),\n            max_size,\n        }\n    }\n\n    fn get(&self, key: &K) -&gt; Option&lt;&V&gt; {\n        self.entries.get(key)\n    }\n}\n\nDynamic Dispatch (dyn Trait)\nWhen you need runtime polymorphism (like virtual functions in C++ / interface references in Java):\nfn log_all(items: &[&dyn Describable]) {\n    for item in items {\n        println!(\"{}\", item.describe());\n    }\n}\n\n// Or with Box for owned trait objects\nfn get_backend(config: &str) -&gt; Box&lt;dyn StorageTrait&gt; {\n    match config {\n        \"ceph\" =&gt; Box::new(CephBackend::new()),\n        \"local\" =&gt; Box::new(LocalBackend::new()),\n        _ =&gt; panic!(\"unknown backend\"),\n    }\n}\ndyn Trait = vtable dispatch (like C++ virtual). impl Trait / generics = monomorphization (like C++ templates). Prefer generics for performance; use dyn when you need heterogeneous collections or runtime selection."
  },
  {
    "objectID": "rust_primer.html#string-types-cheat-sheet",
    "href": "rust_primer.html#string-types-cheat-sheet",
    "title": "Rust Primer for C++/Java Developers",
    "section": "15. String Types Cheat Sheet",
    "text": "15. String Types Cheat Sheet\nThis trips up every newcomer:\n\n\n\n\n\n\n\n\nType\nWhat\nAnalogy\n\n\n\n\nString\nOwned, heap-allocated, growable\nstd::string\n\n\n&str\nBorrowed slice (view into a String or literal)\nstd::string_view / const char*\n\n\n\nlet owned: String = String::from(\"hello\");   // or \"hello\".to_string()\nlet slice: &str = &owned;                     // borrow as slice\nlet literal: &str = \"hello\";                  // string literal (lives in binary, static lifetime)\n\n// Convert between them:\nlet s: String = \"hello\".to_string();          // &str ‚Üí String\nlet s: String = String::from(\"hello\");        // same\nlet s: &str = &owned;                         // String ‚Üí &str (via Deref coercion)\nlet s: &str = owned.as_str();                 // explicit\nRule of thumb: Functions should take &str as input (accepts both) and return String if they‚Äôre creating new string data."
  },
  {
    "objectID": "rust_primer.html#testing",
    "href": "rust_primer.html#testing",
    "title": "Rust Primer for C++/Java Developers",
    "section": "16. Testing",
    "text": "16. Testing\nTests live right next to the code. No separate test files needed (though you can have them).\npub fn add(a: i32, b: i32) -&gt; i32 {\n    a + b\n}\n\n#[cfg(test)]                       // only compiled during `cargo test`\nmod tests {\n    use super::*;                  // import from parent module\n\n    #[test]\n    fn test_add() {\n        assert_eq!(add(2, 3), 5);\n    }\n\n    #[test]\n    fn test_add_negative() {\n        assert_eq!(add(-1, 1), 0);\n    }\n\n    #[test]\n    #[should_panic(expected = \"overflow\")]\n    fn test_overflow() {\n        // test that something panics\n    }\n}\nIntegration tests go in tests/ directory at the project root."
  },
  {
    "objectID": "rust_primer.html#practical-patterns-youll-use-daily",
    "href": "rust_primer.html#practical-patterns-youll-use-daily",
    "title": "Rust Primer for C++/Java Developers",
    "section": "17. Practical Patterns You‚Äôll Use Daily",
    "text": "17. Practical Patterns You‚Äôll Use Daily\n\nBuilder Pattern (common in Rust APIs)\nlet client = ClientBuilder::new()\n    .timeout(Duration::from_secs(30))\n    .max_retries(3)\n    .base_url(\"https://api.example.com\")\n    .build()?;\n\n\nFrom / Into Conversions\nimpl From&lt;NodeConfig&gt; for Server {\n    fn from(config: NodeConfig) -&gt; Self {\n        Server::new(config.hostname, config.cores)\n    }\n}\n\n// Now you can do:\nlet server: Server = config.into();\nlet server = Server::from(config);\n\n\nType Aliases\ntype Result&lt;T&gt; = std::result::Result&lt;T, MyError&gt;;   // common in libraries\ntype NodeMap = HashMap&lt;String, Vec&lt;Server&gt;&gt;;\n\n\nDestructuring\nlet (x, y, z) = (1, 2, 3);\n\nlet Server { hostname, cpu_cores, .. } = &server;   // struct destructuring\n\nif let Ok(config) = load_config(\"path\") {\n    // use config\n}"
  },
  {
    "objectID": "rust_primer.html#key-differences-cheat-sheet",
    "href": "rust_primer.html#key-differences-cheat-sheet",
    "title": "Rust Primer for C++/Java Developers",
    "section": "18. Key Differences Cheat Sheet",
    "text": "18. Key Differences Cheat Sheet\n\n\n\n\n\n\n\n\n\nConcept\nC++\nJava\nRust\n\n\n\n\nMemory\nManual / RAII\nGC\nOwnership + RAII\n\n\nNull\nnullptr\nnull\nOption&lt;T&gt; (no null)\n\n\nErrors\nExceptions / error codes\nExceptions\nResult&lt;T, E&gt;\n\n\nInheritance\nClass hierarchy\nClass hierarchy\nNone (use traits + composition)\n\n\nPolymorphism\nVirtual functions\nInterfaces\nTraits (dyn or generics)\n\n\nConcurrency safety\n‚Äúbe careful‚Äù\n‚Äúbe careful‚Äù\nCompile-time enforcement\n\n\nPackage manager\n???\nMaven/Gradle\nCargo (it‚Äôs great)\n\n\nStrings\nstd::string / const char*\nString\nString / &str\n\n\nDefault mutability\nMutable\nMutable (refs)\nImmutable"
  },
  {
    "objectID": "rust_primer.html#quick-syntax-reference",
    "href": "rust_primer.html#quick-syntax-reference",
    "title": "Rust Primer for C++/Java Developers",
    "section": "19. Quick Syntax Reference",
    "text": "19. Quick Syntax Reference\n// Variables\nlet x = 5;                     // immutable\nlet mut y = 10;                // mutable\nlet z: i64 = 100;             // explicit type\n\n// Functions\nfn add(a: i32, b: i32) -&gt; i32 {\n    a + b                       // no semicolon = return value (expression)\n}\n\n// Conditionals (expressions ‚Äî they return values)\nlet status = if cpu_load &gt; 90 { \"critical\" } else { \"ok\" };\n\n// Loops\nfor item in &collection { }\nfor i in 0..10 { }            // 0 to 9\nfor i in 0..=10 { }           // 0 to 10 (inclusive)\nwhile condition { }\nloop { break value; }          // infinite loop, can return a value\n\n// Printing\nprintln!(\"Hello {name}\");                      // string interpolation\nprintln!(\"Debug: {:?}\", some_struct);           // debug format\nprintln!(\"Pretty debug: {:#?}\", some_struct);   // pretty-printed debug\neprintln!(\"Error: {e}\");                        // print to stderr\n\n// Turbofish (explicit type for generic functions)\nlet nums = \"1,2,3\".split(',').collect::&lt;Vec&lt;&str&gt;&gt;();"
  },
  {
    "objectID": "rust_primer.html#serde-serialization-deserialization",
    "href": "rust_primer.html#serde-serialization-deserialization",
    "title": "Rust Primer for C++/Java Developers",
    "section": "20. Serde: Serialization & Deserialization",
    "text": "20. Serde: Serialization & Deserialization\nSerde is Rust‚Äôs de facto serialization framework. It‚Äôs fast, flexible, and works with JSON, TOML, YAML, MessagePack, and dozens of other formats.\n\nBasic Setup\n# Cargo.toml\n[dependencies]\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"    # for JSON\ntoml = \"0.8\"          # for TOML\n\n\nDerive Macros: The 90% Case\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Serialize, Deserialize)]\nstruct ServerConfig {\n    hostname: String,\n    port: u16,\n    #[serde(default)]                    // use Default::default() if missing\n    enabled: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    description: Option&lt;String&gt;,\n    #[serde(rename = \"maxConnections\")]  // different name in JSON\n    max_connections: u32,\n}\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Deserialize from JSON\n    let json = r#\"{\n        \"hostname\": \"node-01\",\n        \"port\": 8080,\n        \"maxConnections\": 1000\n    }\"#;\n\n    let config: ServerConfig = serde_json::from_str(json)?;\n    println!(\"{:?}\", config);\n\n    // Serialize to JSON\n    let output = serde_json::to_string_pretty(&config)?;\n    println!(\"{output}\");\n\n    Ok(())\n}\n\n\nCommon Serde Attributes\n\n\n\n\n\n\n\nAttribute\nEffect\n\n\n\n\n#[serde(rename = \"name\")]\nUse different field name in serialized form\n\n\n#[serde(rename_all = \"camelCase\")]\nRename all fields (struct-level)\n\n\n#[serde(default)]\nUse Default::default() if field missing\n\n\n#[serde(skip)]\nDon‚Äôt serialize or deserialize this field\n\n\n#[serde(skip_serializing)]\nOnly deserialize, not serialize\n\n\n#[serde(skip_serializing_if = \"Option::is_none\")]\nConditionally skip\n\n\n#[serde(flatten)]\nFlatten nested struct into parent\n\n\n#[serde(with = \"module\")]\nUse custom serialize/deserialize functions\n\n\n#[serde(untagged)]\nFor enums: no tag, tries each variant\n\n\n\n\n\nEnums in Serde\n#[derive(Serialize, Deserialize)]\n#[serde(tag = \"type\")]              // \"internally tagged\"\nenum StorageBackend {\n    #[serde(rename = \"local\")]\n    Local { path: String },\n    #[serde(rename = \"s3\")]\n    S3 { bucket: String, region: String },\n}\n\n// Serializes to: {\"type\": \"local\", \"path\": \"/data\"}\n// vs default (externally tagged): {\"Local\": {\"path\": \"/data\"}}\nEnum representations:\n\nExternally tagged (default): {\"Variant\": {...}}\nInternally tagged (#[serde(tag = \"type\")]): {\"type\": \"Variant\", ...}\nAdjacently tagged (#[serde(tag = \"t\", content = \"c\")]): {\"t\": \"Variant\", \"c\": {...}}\nUntagged (#[serde(untagged)]): tries each variant, no tag\n\n\n\nCustom Serialization\nuse serde::{Serializer, Deserializer};\n\n#[derive(Debug)]\nstruct Timestamp(i64);\n\nimpl Serialize for Timestamp {\n    fn serialize&lt;S: Serializer&gt;(&self, serializer: S) -&gt; Result&lt;S::Ok, S::Error&gt; {\n        // Serialize as ISO 8601 string instead of raw i64\n        let datetime = chrono::DateTime::from_timestamp(self.0, 0).unwrap();\n        serializer.serialize_str(&datetime.to_rfc3339())\n    }\n}\n\n\nReading Config Files\nuse std::fs;\n\n// TOML config file\nfn load_toml_config(path: &str) -&gt; Result&lt;ServerConfig, Box&lt;dyn std::error::Error&gt;&gt; {\n    let contents = fs::read_to_string(path)?;\n    let config: ServerConfig = toml::from_str(&contents)?;\n    Ok(config)\n}\n\n// JSON config file\nfn load_json_config(path: &str) -&gt; Result&lt;ServerConfig, Box&lt;dyn std::error::Error&gt;&gt; {\n    let contents = fs::read_to_string(path)?;\n    let config: ServerConfig = serde_json::from_str(&contents)?;\n    Ok(config)\n}\nPractical tip: Use serde_json::Value for dynamic/unknown JSON structures:\nuse serde_json::Value;\n\nlet v: Value = serde_json::from_str(json)?;\nlet hostname = v[\"hostname\"].as_str().unwrap_or(\"unknown\");"
  },
  {
    "objectID": "rust_primer.html#macros",
    "href": "rust_primer.html#macros",
    "title": "Rust Primer for C++/Java Developers",
    "section": "21. Macros",
    "text": "21. Macros\nMacros are code that writes code. Rust has two types: declarative macros (macro_rules!) and procedural macros.\n\nDeclarative Macros (macro_rules!)\nThese are pattern-matching based. You‚Äôve already used them: println!, vec!, format!.\n// Simple macro\nmacro_rules! say_hello {\n    () =&gt; {\n        println!(\"Hello!\");\n    };\n}\n\n// With arguments\nmacro_rules! create_function {\n    ($name:ident) =&gt; {\n        fn $name() {\n            println!(\"Function {:?} was called\", stringify!($name));\n        }\n    };\n}\n\ncreate_function!(foo);  // creates: fn foo() { ... }\ncreate_function!(bar);  // creates: fn bar() { ... }\n\nfn main() {\n    say_hello!();\n    foo();\n    bar();\n}\n\n\nFragment Types (What You Can Match)\n\n\n\n\n\n\n\n\nFragment\nMatches\nExample\n\n\n\n\n$x:ident\nIdentifier\nfoo, my_var\n\n\n$x:expr\nExpression\n1 + 2, foo()\n\n\n$x:ty\nType\ni32, Vec&lt;String&gt;\n\n\n$x:path\nPath\nstd::collections::HashMap\n\n\n$x:stmt\nStatement\nlet x = 1;\n\n\n$x:block\nBlock\n{ ... }\n\n\n$x:item\nItem (fn, struct, etc.)\nfn foo() {}\n\n\n$x:pat\nPattern\nSome(x), _\n\n\n$x:literal\nLiteral\n42, \"hello\"\n\n\n$x:tt\nToken tree (anything)\nany single token or (...), [...], {...}\n\n\n\n\n\nRepetition\n// The vec! macro (simplified)\nmacro_rules! my_vec {\n    // Match zero or more expressions separated by commas\n    ( $( $elem:expr ),* ) =&gt; {\n        {\n            let mut v = Vec::new();\n            $( v.push($elem); )*    // repeat the push for each element\n            v\n        }\n    };\n}\n\nlet nums = my_vec![1, 2, 3, 4, 5];\nRepetition operators: - * ‚Äî zero or more - + ‚Äî one or more - ? ‚Äî zero or one\n\n\nPractical Example: A Logging Macro\nmacro_rules! log_debug {\n    ($($arg:tt)*) =&gt; {\n        #[cfg(debug_assertions)]\n        {\n            eprintln!(\"[DEBUG {}:{}] {}\", file!(), line!(), format!($($arg)*));\n        }\n    };\n}\n\nfn process_data(x: i32) {\n    log_debug!(\"Processing value: {}\", x);\n    // In release builds, this compiles to nothing\n}\n\n\nProcedural Macros (Brief Overview)\nProc macros are more powerful but require a separate crate. There are three types:\n\nDerive macros ‚Äî #[derive(MyTrait)]\nAttribute macros ‚Äî #[my_attribute]\nFunction-like macros ‚Äî my_macro!(...)\n\n// Using a derive macro (you use these all the time)\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct MyStruct { ... }\n\n// Using an attribute macro\n#[tokio::main]           // transforms fn main into async runtime setup\nasync fn main() { ... }\n\n#[test]                  // marks function as a test\nfn test_something() { ... }\nWriting proc macros requires the proc_macro, syn (parsing), and quote (code generation) crates:\n// In a separate crate with proc-macro = true in Cargo.toml\nuse proc_macro::TokenStream;\nuse quote::quote;\nuse syn::{parse_macro_input, DeriveInput};\n\n#[proc_macro_derive(MyDebug)]\npub fn my_debug_derive(input: TokenStream) -&gt; TokenStream {\n    let input = parse_macro_input!(input as DeriveInput);\n    let name = input.ident;\n\n    let expanded = quote! {\n        impl std::fmt::Debug for #name {\n            fn fmt(&self, f: &mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {\n                write!(f, stringify!(#name))\n            }\n        }\n    };\n\n    TokenStream::from(expanded)\n}\n\n\nWhen to Use Macros\nUse declarative macros (macro_rules!) for: - Reducing repetitive code patterns - DSLs (domain-specific languages) - Compile-time code generation - Variadic functions (Rust doesn‚Äôt have them otherwise)\nUse derive macros when: - You need to implement traits automatically - The implementation follows a pattern based on struct/enum structure\nPrefer functions over macros when: - A regular function would work (macros are harder to debug) - You don‚Äôt need compile-time code generation"
  },
  {
    "objectID": "rust_primer.html#unsafe-rust",
    "href": "rust_primer.html#unsafe-rust",
    "title": "Rust Primer for C++/Java Developers",
    "section": "22. Unsafe Rust",
    "text": "22. Unsafe Rust\nunsafe lets you do five things the compiler can‚Äôt verify. It doesn‚Äôt turn off the borrow checker ‚Äî it just gives you access to low-level operations.\n\nWhat Unsafe Allows\n\nDereference raw pointers\nCall unsafe functions/methods\nAccess/modify mutable static variables\nImplement unsafe traits\nAccess fields of unions\n\n\n\nRaw Pointers\nfn main() {\n    let mut x = 10;\n\n    // Creating raw pointers is safe\n    let ptr_immut: *const i32 = &x;       // immutable raw pointer\n    let ptr_mut: *mut i32 = &mut x;       // mutable raw pointer\n\n    // Dereferencing requires unsafe\n    unsafe {\n        println!(\"Value: {}\", *ptr_immut);\n        *ptr_mut = 20;\n        println!(\"New value: {}\", *ptr_mut);\n    }\n}\nRaw pointers: - Are allowed to be null - Don‚Äôt guarantee they point to valid memory - Can have multiple mutable pointers to the same location - Don‚Äôt have lifetimes (no borrow checker enforcement)\n\n\nUnsafe Functions\n// Marked unsafe because caller must ensure preconditions\nunsafe fn dangerous(ptr: *const i32) -&gt; i32 {\n    *ptr    // could crash if ptr is null or invalid\n}\n\nfn main() {\n    let x = 42;\n    let result = unsafe { dangerous(&x) };\n    println!(\"{result}\");\n}\n\n\nSafe Abstractions Over Unsafe Code\nThis is the key pattern: wrap unsafe code in safe interfaces.\nfn split_at_mut(values: &mut [i32], mid: usize) -&gt; (&mut [i32], &mut [i32]) {\n    let len = values.len();\n    let ptr = values.as_mut_ptr();\n\n    assert!(mid &lt;= len);  // safety check\n\n    unsafe {\n        (\n            std::slice::from_raw_parts_mut(ptr, mid),\n            std::slice::from_raw_parts_mut(ptr.add(mid), len - mid),\n        )\n    }\n}\n\n// The function itself is safe because:\n// 1. We validated mid &lt;= len\n// 2. The two slices don't overlap\n// 3. The lifetimes are correctly tied to the input\n\n\nCalling C Functions (FFI)\n// Link to the C standard library\nextern \"C\" {\n    fn abs(input: i32) -&gt; i32;\n    fn strlen(s: *const i8) -&gt; usize;\n}\n\nfn main() {\n    unsafe {\n        println!(\"abs(-5) = {}\", abs(-5));\n    }\n}\nFor proper FFI, use the libc crate and consider bindgen for generating bindings automatically.\n\n\nMutable Static Variables\nstatic mut COUNTER: u32 = 0;\n\nfn increment() {\n    unsafe {\n        COUNTER += 1;\n    }\n}\n\nfn main() {\n    increment();\n    increment();\n    unsafe {\n        println!(\"COUNTER: {COUNTER}\");\n    }\n}\nAvoid mutable statics ‚Äî they‚Äôre a source of data races. Use AtomicU32, Mutex, or lazy_static! / once_cell instead.\n\n\nWhen to Use Unsafe\nLegitimate uses: - FFI (calling C libraries) - Performance-critical code where you can prove safety but the compiler can‚Äôt - Implementing low-level data structures (linked lists, trees with parent pointers) - Interfacing with hardware / OS APIs\nGuidelines: 1. Keep unsafe blocks as small as possible 2. Document the invariants you‚Äôre maintaining 3. Wrap unsafe code in safe abstractions 4. Use #[deny(unsafe_code)] crate-wide, then allow it explicitly where needed\n// Good: minimal unsafe, safe wrapper\npub fn get_unchecked(slice: &[i32], index: usize) -&gt; i32 {\n    // SAFETY: Caller must ensure index &lt; slice.len()\n    unsafe { *slice.get_unchecked(index) }\n}\n\n// Better: don't expose unsafe at all\npub fn get_or_default(slice: &[i32], index: usize, default: i32) -&gt; i32 {\n    slice.get(index).copied().unwrap_or(default)\n}\n\n\nCommon Unsafe Patterns to Know\n// Transmute: reinterpret bytes (very dangerous)\nlet x: i32 = unsafe { std::mem::transmute([0u8, 0, 0, 1]) };\n\n// ManuallyDrop: prevent destructor from running\nlet mut s = std::mem::ManuallyDrop::new(String::from(\"hello\"));\n// s will NOT be dropped when it goes out of scope\n\n// MaybeUninit: uninitialized memory\nuse std::mem::MaybeUninit;\nlet mut x = MaybeUninit::&lt;i32&gt;::uninit();\nunsafe {\n    x.as_mut_ptr().write(42);\n    let val = x.assume_init();  // now safe to use\n}\nBottom line: Most Rust code is 100% safe. You can write entire applications without a single unsafe. But when you need it ‚Äî for FFI, hardware access, or performance ‚Äî it‚Äôs there, and the explicit marking makes it easy to audit."
  },
  {
    "objectID": "rust_primer.html#the-newtype-pattern",
    "href": "rust_primer.html#the-newtype-pattern",
    "title": "Rust Primer for C++/Java Developers",
    "section": "23. The Newtype Pattern",
    "text": "23. The Newtype Pattern\nNewtypes are single-field tuple structs that wrap another type. They provide type safety with zero runtime cost.\n\nBasic Newtype\n// Without newtypes: easy to mix up arguments\nfn create_server(rack_id: String, host_id: String, port: u32) { ... }\n\ncreate_server(host_id, rack_id, port);  // Oops! Swapped arguments, compiles fine\n\n// With newtypes: compile-time safety\npub struct RackId(pub String);\npub struct HostId(pub String);\npub struct Port(pub u32);\n\nfn create_server(rack: RackId, host: HostId, port: Port) { ... }\n\ncreate_server(host_id, rack_id, port);  // COMPILE ERROR: expected RackId, found HostId\n\n\nImplementing Common Traits\n#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]\npub struct UserId(pub String);\n\nimpl UserId {\n    pub fn new(id: impl Into&lt;String&gt;) -&gt; Self {\n        UserId(id.into())\n    }\n\n    pub fn as_str(&self) -&gt; &str {\n        &self.0\n    }\n}\n\nimpl std::fmt::Display for UserId {\n    fn fmt(&self, f: &mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {\n        write!(f, \"{}\", self.0)\n    }\n}\n\n// Allow easy conversion from string types\nimpl&lt;S: Into&lt;String&gt;&gt; From&lt;S&gt; for UserId {\n    fn from(s: S) -&gt; Self {\n        UserId(s.into())\n    }\n}\n\n\nMeasurement Types\nNewtypes shine for units of measurement:\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct Meters(pub u32);\n\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub struct Feet(pub u32);\n\n// Can't accidentally add meters to feet\nimpl std::ops::Add for Meters {\n    type Output = Self;\n    fn add(self, other: Self) -&gt; Self {\n        Meters(self.0 + other.0)\n    }\n}\n\n// Implement Sum for iterator collection\nimpl std::iter::Sum for Meters {\n    fn sum&lt;I: Iterator&lt;Item = Self&gt;&gt;(iter: I) -&gt; Self {\n        Meters(iter.map(|m| m.0).sum())\n    }\n}\n\n// Now you can do:\nlet distances = vec![Meters(10), Meters(20), Meters(30)];\nlet total: Meters = distances.into_iter().sum();  // Meters(60)\n\n\nDeref for Transparent Access\nUse Deref sparingly ‚Äî it provides transparent access to the inner type:\nuse std::ops::Deref;\n\npub struct Email(String);\n\nimpl Deref for Email {\n    type Target = str;\n    fn deref(&self) -&gt; &str {\n        &self.0\n    }\n}\n\nlet email = Email(\"user@example.com\".into());\nprintln!(\"Length: {}\", email.len());  // Can call str methods directly\nWarning: Deref weakens type safety. Use it when the newtype is truly just a wrapper with the same semantics, not when you want distinct behavior."
  },
  {
    "objectID": "rust_primer.html#phantomdata-and-marker-traits",
    "href": "rust_primer.html#phantomdata-and-marker-traits",
    "title": "Rust Primer for C++/Java Developers",
    "section": "24. PhantomData and Marker Traits",
    "text": "24. PhantomData and Marker Traits\nThese are advanced type-level programming tools for encoding constraints without runtime cost.\n\nPhantomData\nPhantomData&lt;T&gt; tells the compiler ‚Äúthis type logically contains a T‚Äù without actually storing one. Used when types need to track type information at compile time.\nuse std::marker::PhantomData;\n\n// A typed ID that tracks what entity it refers to\npub struct Id&lt;T&gt; {\n    value: u64,\n    _phantom: PhantomData&lt;T&gt;,  // zero-sized, no runtime cost\n}\n\nstruct User;\nstruct Order;\n\nimpl&lt;T&gt; Id&lt;T&gt; {\n    fn new(value: u64) -&gt; Self {\n        Id { value, _phantom: PhantomData }\n    }\n}\n\nlet user_id: Id&lt;User&gt; = Id::new(42);\nlet order_id: Id&lt;Order&gt; = Id::new(42);\n\n// These are different types despite having the same value!\n// fn process_user(id: Id&lt;User&gt;) { ... }\n// process_user(order_id);  // COMPILE ERROR\n\n\nType-Safe Links with PhantomData\nuse std::marker::PhantomData;\n\npub trait Port: Clone {}\npub trait Cable: Clone {}\n\n// A link between two ports via a cable, fully type-safe\npub struct Link&lt;PA: Port, C: Cable, PB: Port&gt; {\n    port_a_id: String,\n    port_b_id: String,\n    _phantom: PhantomData&lt;(PA, C, PB)&gt;,\n}\n\nimpl&lt;PA: Port, C: Cable, PB: Port&gt; Link&lt;PA, C, PB&gt; {\n    pub fn new(a: String, b: String) -&gt; Self {\n        Link {\n            port_a_id: a,\n            port_b_id: b,\n            _phantom: PhantomData,\n        }\n    }\n}\n\n\nMarker Traits\nEmpty traits that categorize types without adding behavior:\n// Marker traits for action targeting\npub trait ServerHostMarker {}\npub trait SwitchHostMarker {}\n\n// Only servers implement this\nimpl ServerHostMarker for ServerHost {}\n\n// Only switches implement this\nimpl SwitchHostMarker for SwitchHost {}\n\n// Action that only works on servers\npub struct InstallOS&lt;H: ServerHostMarker&gt; {\n    os_image: String,\n    _phantom: PhantomData&lt;H&gt;,\n}\n\n// Universal marker with blanket implementation\npub trait UniversalHostMarker {}\nimpl&lt;H&gt; UniversalHostMarker for H {}  // All types get this\n\n\nCommon Standard Library Markers\n\n\n\nMarker\nMeaning\n\n\n\n\nSend\nSafe to transfer between threads\n\n\nSync\nSafe to share references between threads\n\n\nSized\nHas known size at compile time (implicit bound)\n\n\nUnpin\nCan be safely moved after being pinned\n\n\nCopy\nCan be implicitly copied (bitwise)"
  },
  {
    "objectID": "rust_primer.html#downcasting-trait-objects",
    "href": "rust_primer.html#downcasting-trait-objects",
    "title": "Rust Primer for C++/Java Developers",
    "section": "25. Downcasting Trait Objects",
    "text": "25. Downcasting Trait Objects\nWhen you have a dyn Trait and need to get the concrete type back, use the Any trait.\n\nThe Pattern\nuse std::any::Any;\n\npub trait Host: std::fmt::Debug + Send + Sync + Any {\n    fn hostname(&self) -&gt; &str;\n\n    // Required for downcasting\n    fn as_any(&self) -&gt; &dyn Any;\n    fn as_any_mut(&mut self) -&gt; &mut dyn Any;\n}\n\n#[derive(Debug)]\npub struct ServerHost {\n    pub hostname: String,\n    pub cpu_cores: u32,\n}\n\nimpl Host for ServerHost {\n    fn hostname(&self) -&gt; &str {\n        &self.hostname\n    }\n\n    fn as_any(&self) -&gt; &dyn Any {\n        self\n    }\n\n    fn as_any_mut(&mut self) -&gt; &mut dyn Any {\n        self\n    }\n}\n\n#[derive(Debug)]\npub struct SwitchHost {\n    pub hostname: String,\n    pub port_count: u32,\n}\n\nimpl Host for SwitchHost {\n    fn hostname(&self) -&gt; &str {\n        &self.hostname\n    }\n\n    fn as_any(&self) -&gt; &dyn Any {\n        self\n    }\n\n    fn as_any_mut(&mut self) -&gt; &mut dyn Any {\n        self\n    }\n}\n\n\nUsing Downcasting\nfn process_host(host: &dyn Host) {\n    println!(\"Processing: {}\", host.hostname());\n\n    // Try to downcast to ServerHost\n    if let Some(server) = host.as_any().downcast_ref::&lt;ServerHost&gt;() {\n        println!(\"  Server with {} cores\", server.cpu_cores);\n    }\n    // Try to downcast to SwitchHost\n    else if let Some(switch) = host.as_any().downcast_ref::&lt;SwitchHost&gt;() {\n        println!(\"  Switch with {} ports\", switch.port_count);\n    }\n}\n\nfn main() {\n    let hosts: Vec&lt;Box&lt;dyn Host&gt;&gt; = vec![\n        Box::new(ServerHost { hostname: \"srv-01\".into(), cpu_cores: 64 }),\n        Box::new(SwitchHost { hostname: \"sw-01\".into(), port_count: 48 }),\n    ];\n\n    for host in &hosts {\n        process_host(host.as_ref());\n    }\n}\n\n\nCloning Trait Objects\nStandard Clone doesn‚Äôt work with trait objects. Use the clone_box pattern:\npub trait Host: std::fmt::Debug {\n    fn clone_box(&self) -&gt; Box&lt;dyn Host&gt;;\n    // ... other methods\n}\n\nimpl Clone for Box&lt;dyn Host&gt; {\n    fn clone(&self) -&gt; Self {\n        self.clone_box()\n    }\n}\n\nimpl Host for ServerHost {\n    fn clone_box(&self) -&gt; Box&lt;dyn Host&gt; {\n        Box::new(self.clone())\n    }\n}"
  },
  {
    "objectID": "rust_primer.html#cargo-workspaces",
    "href": "rust_primer.html#cargo-workspaces",
    "title": "Rust Primer for C++/Java Developers",
    "section": "26. Cargo Workspaces",
    "text": "26. Cargo Workspaces\nFor larger projects, organize multiple related crates in a workspace.\n\nWorkspace Structure\nmy-project/\n‚îú‚îÄ‚îÄ Cargo.toml              # Workspace root\n‚îú‚îÄ‚îÄ crates/\n‚îÇ   ‚îú‚îÄ‚îÄ my-core/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/lib.rs\n‚îÇ   ‚îú‚îÄ‚îÄ my-cli/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/main.rs\n‚îÇ   ‚îî‚îÄ‚îÄ my-macros/\n‚îÇ       ‚îú‚îÄ‚îÄ Cargo.toml\n‚îÇ       ‚îî‚îÄ‚îÄ src/lib.rs\n‚îî‚îÄ‚îÄ examples/\n    ‚îî‚îÄ‚îÄ demo/\n        ‚îú‚îÄ‚îÄ Cargo.toml\n        ‚îî‚îÄ‚îÄ src/main.rs\n\n\nRoot Cargo.toml\n[workspace]\nresolver = \"2\"\nmembers = [\n    \"crates/my-core\",\n    \"crates/my-cli\",\n    \"crates/my-macros\",\n    \"examples/demo\",\n]\n\n# Shared dependencies with consistent versions\n[workspace.dependencies]\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nthiserror = \"1.0\"\nclap = { version = \"4.5\", features = [\"derive\"] }\ntokio = { version = \"1.0\", features = [\"full\"] }\n\n# Internal crates as workspace dependencies\nmy-core = { path = \"crates/my-core\" }\nmy-macros = { path = \"crates/my-macros\" }\n\n\nCrate Cargo.toml\n[package]\nname = \"my-cli\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n# Use workspace versions\nserde.workspace = true\nserde_json.workspace = true\nclap.workspace = true\n\n# Internal dependencies\nmy-core.workspace = true\n\n\nBenefits\n\nShared dependencies: One version across all crates\nUnified builds: cargo build builds everything\nShared target directory: Faster incremental compilation\nCross-crate testing: Run all tests with cargo test --workspace\n\n\n\nProc-Macro Crates\nProcedural macros must be in their own crate:\n# crates/my-macros/Cargo.toml\n[package]\nname = \"my-macros\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[lib]\nproc-macro = true\n\n[dependencies]\nsyn = { version = \"2.0\", features = [\"full\"] }\nquote = \"1.0\"\nproc-macro2 = \"1.0\"\nThen re-export from your main crate:\n// crates/my-core/src/lib.rs\npub use my_macros::my_derive_macro;"
  },
  {
    "objectID": "rust_primer.html#what-to-learn-next",
    "href": "rust_primer.html#what-to-learn-next",
    "title": "Rust Primer for C++/Java Developers",
    "section": "27. What to Learn Next",
    "text": "27. What to Learn Next\nOnce you‚Äôre comfortable with the above, these are the next frontiers:\n\nAsync Rust in depth: Tokio runtime, futures, select!, streams, async traits\nError handling patterns: thiserror for libraries, anyhow for applications\nPinning: Required for self-referential types in async code\nAdvanced FFI: bindgen for auto-generating C bindings, cbindgen for exposing Rust to C\nEmbedded Rust: no_std, bare metal programming\nSIMD and performance: std::simd, profiling with perf, criterion for benchmarks\nThe std docs: Genuinely excellent ‚Äî https://doc.rust-lang.org/std/\n\n\nBest way to learn: Pick a small project (a CLI tool, a simple HTTP server with axum/actix, a file processor) and just build. The compiler errors are famously helpful ‚Äî read them carefully, they usually tell you exactly what to do."
  },
  {
    "objectID": "hdd_tco_summary.html",
    "href": "hdd_tco_summary.html",
    "title": "HDD comparison and 5-year TCO",
    "section": "",
    "text": "Compare three enterprise nearline HDDs for a storage cluster.\nRequirement: 5 PB (5,000 TB decimal) raw capacity already includes replication/EC overhead.\nStorage servers: 24 bays/server, ‚Çπ8‚Äì9L capex per server.\nElectricity: ‚Çπ17/kWh, 24√ó7, 5 years.\nTCO includes: drive capex + server capex + electricity (drive idle + server base idle).\nElectricity model uses vendor datasheet idle watts for drives; server base idle is an explicit assumption."
  },
  {
    "objectID": "hdd_tco_summary.html#context",
    "href": "hdd_tco_summary.html#context",
    "title": "HDD comparison and 5-year TCO",
    "section": "",
    "text": "Compare three enterprise nearline HDDs for a storage cluster.\nRequirement: 5 PB (5,000 TB decimal) raw capacity already includes replication/EC overhead.\nStorage servers: 24 bays/server, ‚Çπ8‚Äì9L capex per server.\nElectricity: ‚Çπ17/kWh, 24√ó7, 5 years.\nTCO includes: drive capex + server capex + electricity (drive idle + server base idle).\nElectricity model uses vendor datasheet idle watts for drives; server base idle is an explicit assumption."
  },
  {
    "objectID": "hdd_tco_summary.html#dc-grade-assessment-short",
    "href": "hdd_tco_summary.html#dc-grade-assessment-short",
    "title": "HDD comparison and 5-year TCO",
    "section": "DC-grade assessment (short)",
    "text": "DC-grade assessment (short)\n\nAll three are data-center/enterprise nearline SKUs on-paper: 24√ó7 duty class, 5-year warranty, MTBF/AFR class typical of DC drives.\nDifferences are mainly capacity per drive (18TB vs 24TB) and minor interface/power/feature details."
  },
  {
    "objectID": "hdd_tco_summary.html#inputs-used",
    "href": "hdd_tco_summary.html#inputs-used",
    "title": "HDD comparison and 5-year TCO",
    "section": "Inputs used",
    "text": "Inputs used\n\nTime horizon: 5 years (hours = 43,800)\nServer base idle (excluding disks): 400W/server (assumption; see sensitivity)\nDrive idle watts (from datasheets):\n\nSeagate Exos X24 ST24000NM007H: 6.5W idle; https://www.seagate.com/content/dam/seagate/en/content-fragments/products/datasheets/exos-x24/exos-x24-DS2080-2307US-en_US.pdf\nWD Ultrastar DC HC590 24TB (SKU 0F59373): 5.9W idle; https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/product/data-center-drives/ultrastar-dc-hc500-series/data-sheet-ultrastar-dc-hc590.pdf\nSeagate Exos X20 ST18000NM000D: 5.8W idle; https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-channel-DS2080-2111GB-en_EM.pdf"
  },
  {
    "objectID": "hdd_tco_summary.html#drive-counts-and-server-counts-to-reach-5000-tb",
    "href": "hdd_tco_summary.html#drive-counts-and-server-counts-to-reach-5000-tb",
    "title": "HDD comparison and 5-year TCO",
    "section": "Drive counts and server counts to reach ‚â•5,000 TB",
    "text": "Drive counts and server counts to reach ‚â•5,000 TB\n\nFormulae:\n\ndrives = ceil(5000 / TB_per_drive)\nservers = ceil(drives / 24)"
  },
  {
    "objectID": "hdd_tco_summary.html#year-cost-model",
    "href": "hdd_tco_summary.html#year-cost-model",
    "title": "HDD comparison and 5-year TCO",
    "section": "5-year cost model",
    "text": "5-year cost model\n\nElectricity:\n\nkWh = (Watts/1000) * hours\npower_cost = kWh * ‚Çπ17\n\n5-year TCO range (server capex ‚Çπ8‚Äì9L):\n\nTCO = drive_capex + server_capex + drive_power_cost + server_power_cost"
  },
  {
    "objectID": "hdd_tco_summary.html#results-5-pb-raw",
    "href": "hdd_tco_summary.html#results-5-pb-raw",
    "title": "HDD comparison and 5-year TCO",
    "section": "Results (5 PB raw)",
    "text": "Results (5 PB raw)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrive\nTB/drive\nDrives\nServers (24-bay)\nDrive CAPEX\nServer CAPEX\n5y drive power\n5y server power (@400W)\n5y TCO\n‚Çπ/TB (drive-only)\n\n\n\n\nSeagate Exos X24 ST24000NM007H\n24\n209\n9\n97.19L\n72.00L‚Äì81.00L\n10.12L\n26.81L\n206.11L‚Äì215.11L\n1937.5\n\n\nWD Ultrastar DC HC590 24TB (SKU 0F59373)\n24\n209\n9\n96.56L\n72.00L‚Äì81.00L\n9.18L\n26.81L\n204.55L‚Äì213.55L\n1925.0\n\n\nSeagate Exos X20 ST18000NM000D\n18\n278\n12\n80.62L\n96.00L‚Äì108.00L\n12.01L\n35.74L\n224.37L‚Äì236.37L\n1611.11"
  },
  {
    "objectID": "hdd_tco_summary.html#findings",
    "href": "hdd_tco_summary.html#findings",
    "title": "HDD comparison and 5-year TCO",
    "section": "Findings",
    "text": "Findings\n\nArchitecture dominates: with ‚Çπ8‚Äì9L per 24-bay server, the 18TB option requires 12 servers vs 9 servers for 24TB drives.\nThat extra server capex (and base power) outweighs the 18TB drive‚Äôs lower ‚Çπ/TB.\nBest 5-year value (your constraints): WD Ultrastar DC HC590 24TB.\nThe two 24TB options are close; HC590 wins slightly on your pricing and lower drive idle watts."
  },
  {
    "objectID": "hdd_tco_summary.html#sensitivity-server-idle-power",
    "href": "hdd_tco_summary.html#sensitivity-server-idle-power",
    "title": "HDD comparison and 5-year TCO",
    "section": "Sensitivity (server idle power)",
    "text": "Sensitivity (server idle power)\n\nExtra electricity per +100W per server over 5 years: ‚Çπ74,460 (~0.74L).\nFor 9 servers: +100W/server adds ‚Çπ670,140 (~6.70L).\nFor 12 servers: +100W/server adds ‚Çπ893,520 (~8.94L).\nIf your servers idle significantly above 400W, the penalty for needing more servers increases, which further favors 24TB drives."
  },
  {
    "objectID": "hdd_tco_summary.html#what-this-tco-excludes-important",
    "href": "hdd_tco_summary.html#what-this-tco-excludes-important",
    "title": "HDD comparison and 5-year TCO",
    "section": "What this TCO excludes (important)",
    "text": "What this TCO excludes (important)\n\nCooling overhead (PUE), rack space cost, networking/HBA costs, spares inventory, and operational labor.\nIf you provide measured wall power per storage server (idle) with disks installed and any PUE target, the power part can be tightened."
  },
  {
    "objectID": "hdd_tco_summary.html#datasheet-links",
    "href": "hdd_tco_summary.html#datasheet-links",
    "title": "HDD comparison and 5-year TCO",
    "section": "Datasheet links",
    "text": "Datasheet links\n\nSeagate Exos X24 ST24000NM007H: https://www.seagate.com/content/dam/seagate/en/content-fragments/products/datasheets/exos-x24/exos-x24-DS2080-2307US-en_US.pdf\nWD Ultrastar DC HC590 24TB (SKU 0F59373): https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/product/data-center-drives/ultrastar-dc-hc500-series/data-sheet-ultrastar-dc-hc590.pdf\nSeagate Exos X20 ST18000NM000D: https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-channel-DS2080-2111GB-en_EM.pdf"
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html",
    "href": "ampereone_m_vs_amd_epyc.html",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "",
    "text": "AmpereOne M product brief: https://amperecomputing.com/briefs/ampereone-m-product-brief\nGIGABYTE R1A3-T40-AAV1 (your link may geo-block; this mirror usually works): https://www.gigabyte.com/il/Enterprise/Rack-Server/R1A3-T40-AAV1\nAMD EPYC 9004 series datasheet (model table includes 9554/9654): https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/datasheets/amd-epyc-9004-series-processors-datasheet.pdf"
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#sources-used",
    "href": "ampereone_m_vs_amd_epyc.html#sources-used",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "",
    "text": "AmpereOne M product brief: https://amperecomputing.com/briefs/ampereone-m-product-brief\nGIGABYTE R1A3-T40-AAV1 (your link may geo-block; this mirror usually works): https://www.gigabyte.com/il/Enterprise/Rack-Server/R1A3-T40-AAV1\nAMD EPYC 9004 series datasheet (model table includes 9554/9654): https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/datasheets/amd-epyc-9004-series-processors-datasheet.pdf"
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#executive-summary-what-changes-vs-your-current-epyc-nodes",
    "href": "ampereone_m_vs_amd_epyc.html#executive-summary-what-changes-vs-your-current-epyc-nodes",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Executive summary (what changes vs your current EPYC nodes)",
    "text": "Executive summary (what changes vs your current EPYC nodes)\nYou‚Äôre comparing two very different approaches to ‚Äúcore dense‚Äù:\n\nAMD EPYC 9554 / 9654 (x86_64 + SMT): fewer cores than Ampere‚Äôs top-end SKUs, but 2 threads/core, strong per-core performance, and more I/O (128 PCIe Gen5 lanes).\nAmpereOne M (Armv8.6+, single-threaded cores): many single-threaded cores with 12-channel DDR5-5600, and a platform story optimized for predictable, scale-out throughput ‚Äî but fewer I/O lanes (96 PCIe Gen5) and an Arm software/ops compatibility tax if your stack isn‚Äôt already Arm-friendly."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#at-a-glance-specs-socket-level",
    "href": "ampereone_m_vs_amd_epyc.html#at-a-glance-specs-socket-level",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "At-a-glance specs (socket-level)",
    "text": "At-a-glance specs (socket-level)\n\n\n\n\n\n\n\n\n\nItem\nAMD EPYC 9554\nAMD EPYC 9654\nAmpereOne M (family)\n\n\n\n\nISA / platform\nx86_64 (SP5)\nx86_64 (SP5)\nArmv8.6+ (SoC)\n\n\nCores / threads\n64 / 128\n96 / 192\n96‚Äì192 / 96‚Äì192 (single-thread)\n\n\nBase / boost (GHz)\n3.10 / 3.75\n2.40 / 3.70\nModel-dependent (2.6‚Äì3.6 shown)\n\n\nDefault / usage power\n360W (default TDP)\n360W (default TDP)\n239‚Äì348W ‚ÄúUsage Power*‚Äù by model (brief)\n\n\nMemory channels\n12\n12\n12\n\n\nMax DDR5 speed (1DPC)\n4800 MT/s\n4800 MT/s\n5600 MT/s (brief)\n\n\nPeak memory bandwidth\n460.8 GB/s (datasheet)\n460.8 GB/s (datasheet)\n~537.6 GB/s (12 √ó 5600 MT/s √ó 8 B; theoretical)\n\n\nPCIe Gen5 lanes\n128\n128\n96 (brief)"
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#pros-cons-amd-epyc-vs-ampereone-m",
    "href": "ampereone_m_vs_amd_epyc.html#pros-cons-amd-epyc-vs-ampereone-m",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Pros / cons: AMD EPYC vs AmpereOne M",
    "text": "Pros / cons: AMD EPYC vs AmpereOne M\n\nAMD EPYC (9554/9654 class)\nPros\n\nSoftware compatibility: x86_64 ‚Äújust works‚Äù for the broadest set of enterprise software, drivers, agents, and proprietary stacks.\nHigh thread count with SMT: if your workload benefits from SMT, you get 2√ó threads/core.\nI/O headroom: 128 lanes of PCIe Gen5 per socket is hard to beat when you need many GPUs / NICs / NVMe.\nMature tuning + observability: generally easier to find established guidance for BIOS, NUMA, perf counters, and vendor ecosystem tooling.\n\nCons\n\nHigher platform power for a given throughput-per-rack goal if you‚Äôre ultimately limited by memory bandwidth efficiency or ‚Äúscale-out core density‚Äù.\nLicense cost sensitivity for software priced per core / per socket can dominate your economics (depends on your stack).\n\n\n\nAmpereOne M\nPros\n\nCore density: 96‚Äì192 single-threaded cores per socket can increase throughput-per-rack for workloads that scale well with more independent threads (stateless services, batchy inference, analytics pipelines).\nMemory bandwidth focus: 12-channel DDR5-5600 (brief) is a clear design point for bandwidth-sensitive throughput workloads.\nPredictability: single-threaded cores and the ‚Äúcloud native‚Äù design goal can help with performance consistency (still validate on your stack).\n\nCons\n\nArm migration/ops tax: base images, agents, proprietary libs, and vendor support may lag x86_64 depending on your environment.\nVector ISA / codegen differences: CPU instruction set and library ecosystem differ; some workloads need re-tuning or don‚Äôt have comparable optimized kernels.\nLess PCIe lane budget: 96 PCIe Gen5 lanes (brief) can be a real constraint for GPU/NVMe/NIC-heavy designs."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#where-to-run-amd-vs-where-to-run-arm-practical-placement-guide",
    "href": "ampereone_m_vs_amd_epyc.html#where-to-run-amd-vs-where-to-run-arm-practical-placement-guide",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Where to run AMD vs where to run Arm (practical placement guide)",
    "text": "Where to run AMD vs where to run Arm (practical placement guide)\nThink of Arm (AmpereOne M) as a throughput-per-watt / scale-out play, and AMD EPYC as the lowest-risk default for compatibility + per-thread performance + I/O-heavy nodes.\n\nRun on AMD EPYC (x86_64) when‚Ä¶\n\nYou can‚Äôt fully support Arm in prod yet: any ‚Äúmust have‚Äù component is x86-only (vendor app, security agent, backup agent, monitoring, kernel module, proprietary driver, etc.).\nSingle-thread and tail latency dominate: workloads with strict p95/p99 latency, high per-request CPU, or limited parallelism tend to prefer strong per-core performance.\nYour nodes are I/O-lane constrained: you need lots of PCIe devices per socket (GPUs, multiple high-speed NICs, many NVMe drives, HBAs/DPUs). EPYC‚Äôs 128 PCIe Gen5 lanes is a big advantage.\nYou rely on x86-specific acceleration or tuning: compiled kernels, hand-tuned libraries, or toolchains that assume x86 (common in HPC/EDA/legacy stacks).\nSoftware licensing penalizes high core counts: if your critical software is licensed per core, a 160‚Äì192-core Arm CPU can be economically worse even if it‚Äôs efficient.\n\n\n\nRun on AmpereOne M (Arm) when‚Ä¶\n\nYour stack is ‚Äúrebuildable‚Äù: you can build and ship linux/arm64 artifacts (or use multi-arch container images) for the app and its dependencies.\nThroughput scales with more independent threads: stateless services, web/API tiers, batch workers, message consumers, ETL steps, and many ‚Äúembarrassingly parallel‚Äù jobs.\nYou‚Äôre memory bandwidth sensitive: AmpereOne M is explicitly positioned around 12-channel DDR5-5600 and core density; that‚Äôs often a good fit when you‚Äôre bottlenecked on memory throughput rather than per-thread IPC.\nPower/rack density is a primary constraint: if you‚Äôre hitting rack power limits before space limits, Arm‚Äôs efficiency goals can translate into real capacity gains (validate with your workload).\n\n\n\nTypical ‚Äúgood fits‚Äù (starting assumptions)\n\n\n\n\n\n\n\n\n\nWorkload area\nDefault pick\nWhen Arm usually makes sense\nWhen AMD usually stays better\n\n\n\n\nWeb / API stateless services\nArm candidate\nGo/Java/Node services, containerized, easy CI builds, horizontal scaling\nVery latency-sensitive endpoints or hard-to-rebuild legacy deps\n\n\nBatch workers / queues\nArm candidate\nHigh parallelism, lots of concurrent jobs, predictable CPU work\nJobs with specialized x86-only libraries or heavy per-thread performance needs\n\n\nDatabases / stateful data\nAMD default\nOpen-source DBs you can validate thoroughly; read-heavy fleets; cost/power pressure\nVendor appliances, x86-tuned deployments, extreme p99 requirements, or ‚Äúone big box‚Äù scaling\n\n\nCPU-only inference\ndepends\nMany concurrent small models / pipelines, throughput-oriented serving\nIf per-request latency is strict and model kernels are x86-optimized in your stack\n\n\nGPU-heavy training/inference\nAMD default (for CPU host)\nOnly if the exact GPU + driver + orchestration stack is proven on Arm\nWhen you need maximum PCIe lanes and broadest ecosystem support\n\n\nVirtualization platforms\nAMD default\nIf you run KVM and have full tool/agent support on Arm\nIf you rely on VMware or x86-only guest/management tooling\n\n\n\n\n\nTwo easy deployment patterns\n\nSeparate node pools / fleets:\n\namd64 pool for ‚Äúeverything works‚Äù workloads.\narm64 pool for explicitly qualified workloads.\n\nKubernetes: schedule by architecture:\n\nBuild multi-arch images and use node selectors/affinity (e.g., kubernetes.io/arch=arm64)."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#a-simple-decision-tree-for-amd-or-arm",
    "href": "ampereone_m_vs_amd_epyc.html#a-simple-decision-tree-for-amd-or-arm",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "A simple decision tree for ‚ÄúAMD or Arm?‚Äù",
    "text": "A simple decision tree for ‚ÄúAMD or Arm?‚Äù\n\nCan I run it on Arm without heroics? (build + deps + agents + drivers)\n\nNo ‚Üí AMD\n\nIs it I/O-lane heavy? (lots of PCIe devices per node)\n\nYes ‚Üí AMD\n\nIs single-thread / p99 critical and hard to parallelize?\n\nYes ‚Üí AMD\n\nOtherwise, does it scale with many independent threads and is power/cost a concern?\n\nYes ‚Üí Arm candidate, then validate perf/W and perf/$ in a POC"
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#architecture-basics-x86_64-amd-epyc-vs-arm64-ampereone-m",
    "href": "ampereone_m_vs_amd_epyc.html#architecture-basics-x86_64-amd-epyc-vs-arm64-ampereone-m",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Architecture basics: x86_64 (AMD EPYC) vs Arm64 (AmpereOne M)",
    "text": "Architecture basics: x86_64 (AMD EPYC) vs Arm64 (AmpereOne M)\nAt a high level, both are modern 64-bit server CPUs running Linux, but they differ in the instruction set, ecosystem assumptions, and some performance/optimization ‚Äúdefaults‚Äù.\n\n1) Instruction set (ISA) and ABI\n\nAMD EPYC runs x86_64 binaries (ELF x86-64).\nAmpereOne M runs AArch64 / arm64 binaries (ELF aarch64).\n\nThis is the root cause of most ‚Äúdoesn‚Äôt run on Arm‚Äù issues: a prebuilt program compiled for x86_64 cannot execute on Arm64 without recompilation (or emulation, which is usually not acceptable for production performance).\n\n\n2) SIMD / vector extensions (where hidden incompatibilities show up)\nMany ‚ÄúCPU-intensive‚Äù libraries rely on vector instruction sets.\n\nx86 often leans on SSE/AVX/AVX2/AVX-512.\nArm commonly uses NEON (and sometimes SVE/SVE2 depending on CPU and build targets).\n\nIf an application (or a dependency) ships x86-only optimized code paths (e.g., AVX-512) and doesn‚Äôt provide Arm equivalents, you can see: - ‚ÄúIllegal instruction‚Äù crashes (if runtime dispatch is wrong), - or it compiles/runs but performs much worse (falls back to scalar code).\n\n\n3) Threads and scheduling assumptions\n\nYour EPYC nodes use SMT (2 threads/core), so software often sees ‚Äúmore CPUs‚Äù than physical cores.\nAmpereOne M cores are positioned as single-threaded cores (1 thread/core).\n\nApplications that were tuned assuming SMT behavior (thread pool sizing, CPU pinning, latency isolation) may need re-tuning on Arm even if they are ‚Äúsupported‚Äù.\n\n\n4) Platform model differences\n\nEPYC is a classic server CPU platform (CPU + chipset/IOD, SP5 ecosystem).\nAmpereOne M is more SoC-like in how the platform is built and validated (still standard server components, but a different vendor ecosystem).\n\nPractically, this affects out-of-tree drivers, BIOS/firmware tooling, and vendor-provided management integrations."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#what-kinds-of-applications-might-not-have-support-on-arm",
    "href": "ampereone_m_vs_amd_epyc.html#what-kinds-of-applications-might-not-have-support-on-arm",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "What kinds of applications might not have support on Arm?",
    "text": "What kinds of applications might not have support on Arm?\nThe pattern is: anything delivered only as an x86_64 binary (or relying on x86-only kernel/user-space components).\n\n1) Closed-source / proprietary binaries\n\nCommercial software distributed as a single x86_64 build (no Arm download).\n‚ÄúAppliance-like‚Äù stacks where the vendor only certifies x86_64.\nLegacy enterprise software that embeds x86 native components.\n\n\n\n2) Kernel modules and low-level agents\nThese are common early blockers: - Endpoint/security agents (EDR), DLP, vulnerability scanners - Backup agents - Storage/network drivers, RDMA stacks, DPDK variants (depends), special NIC features - GPU/accelerator drivers and management tooling (often works on Arm in some combos, but don‚Äôt assume)\nIf the solution requires a kernel module, you need explicit Arm64 support for your kernel + distro combination.\n\n\n3) Language ecosystems with native extensions (it ‚Äúinstalls‚Äù until it doesn‚Äôt)\nEven when your application is ‚Äúportable‚Äù (Go/Java/Python/Node), its dependencies may include native code:\n\nPython: pip packages with native wheels may be missing manylinux_aarch64 wheels, forcing source builds (which may fail due to missing toolchains or incompatible C/ASM).\nNode.js: native addons (node-gyp) may lack arm64 prebuilds.\nJava/JVM: the JVM itself supports arm64 well, but JNI libraries might not.\nRust/C/C++: usually fine if you build from source, but some projects have x86-only assembly.\n\n\n\n4) Applications depending on x86-specific optimizations\nExamples: - Media/codec stacks tuned for x86 intrinsics only - Some HPC/EDA toolchains and proprietary numeric libraries - Any codebase with hand-written x86 assembly\n\n\n5) Virtualization and guest OS constraints\n\nIf you need to run x86_64 guests on the same host, Arm servers can‚Äôt do that natively.\nSome virtualization ecosystems (notably VMware) are effectively x86-only in common enterprise deployments."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#fast-ways-to-detect-arm-readiness-before-you-buy-a-lot-of-servers",
    "href": "ampereone_m_vs_amd_epyc.html#fast-ways-to-detect-arm-readiness-before-you-buy-a-lot-of-servers",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Fast ways to detect Arm readiness (before you buy a lot of servers)",
    "text": "Fast ways to detect Arm readiness (before you buy a lot of servers)\n\nContainer images: do they publish linux/arm64?\n\ndocker manifest inspect &lt;image&gt;:&lt;tag&gt; | rg -n \\\"arm64|amd64\\\"\n\nLinux packages: do they ship aarch64/arm64 builds for your distro?\n\nDebian/Ubuntu: check arm64 availability in your repo + vendor repo.\nRHEL-like: check aarch64 RPM availability and kernel module support.\n\nYour own binaries: can you build and run CI for arm64?\n\nuname -m on the target (aarch64 vs x86_64)\nfile &lt;binary&gt; should show aarch64 for Arm targets."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#equivalent-cpu-selection-how-to-map-epyc-95549654-ampereone-m",
    "href": "ampereone_m_vs_amd_epyc.html#equivalent-cpu-selection-how-to-map-epyc-95549654-ampereone-m",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "‚ÄúEquivalent‚Äù CPU selection: how to map EPYC 9554/9654 ‚Üí AmpereOne M",
    "text": "‚ÄúEquivalent‚Äù CPU selection: how to map EPYC 9554/9654 ‚Üí AmpereOne M\nThere isn‚Äôt a single ‚Äúequivalent‚Äù, because AMD cores and Ampere cores are not interchangeable. Pick equivalence based on what you‚Äôre trying to preserve:\n\nThread budget equivalence (how many runnable software threads you want per node)\nPower envelope equivalence (stay within a rack/power budget per node)\nMemory bandwidth equivalence (channels √ó speed, plus workload memory behavior)\nI/O equivalence (PCIe lanes and slot topology)\nSingle-thread / tail-latency equivalence (p99 requirements)\n\n\nPractical mapping shortlist (starting point)\nUse this as a POC shortlist, not a final answer.\n\n\n\n\n\n\n\n\n\nIf you run today‚Ä¶\nWhat you likely value\nAmpereOne M candidates to try\nWhy these are ‚Äúclosest‚Äù\n\n\n\n\nEPYC 9554 (64C/128T, 360W)\nhigher per-core perf, moderate core count, still lots of threads\nA96-36M (96C @ 3.6, 331W), A144-33M (144C @ 3.3, 334W), A144-26M (144C @ 2.6, 239W)\nA96-36M is the ‚Äúfewer cores, higher freq‚Äù end; A144 variants test whether more cores improves throughput under similar or lower power\n\n\nEPYC 9654 (96C/192T, 360W)\nhigh thread count / throughput per node\nA192-32M (192C @ 3.2, 348W), A160-28M (160C @ 2.8, 262W), A192-26M (192C @ 2.6, 278W)\nA192-32M matches the thread budget (192) at similar power; A160/A192-26M are ‚Äúefficiency-first‚Äù comparators\n\n\n\n\n\nOne hard constraint: PCIe lane budget\nIf your current EPYC nodes are built around lane-heavy designs (many NVMe drives, multiple NICs, GPUs, DPUs), treat 128 lanes (EPYC) vs 96 lanes (AmpereOne M) as a first-order design constraint. It can change the entire server bill of materials more than the CPU choice itself."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#gigabyte-r1a3-t40-aav1-notes-ampereone-m-platform",
    "href": "ampereone_m_vs_amd_epyc.html#gigabyte-r1a3-t40-aav1-notes-ampereone-m-platform",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "GIGABYTE R1A3-T40-AAV1 notes (AmpereOne M platform)",
    "text": "GIGABYTE R1A3-T40-AAV1 notes (AmpereOne M platform)\nFrom the vendor spec page, this platform is a 1U, single-socket AmpereOne M server with:\n\n12 DDR5 RDIMM slots (12-channel per CPU)\n4 front hot-swap bays (Gen5 NVMe / SATA / SAS-4 support)\n2 √ó FHHL PCIe Gen5 x16 slots + 2 √ó OCP NIC 3.0 PCIe Gen5 x16 slots\nRedundant power supplies (config shown as Titanium-class)\n\nThis makes it a reasonable evaluation chassis for NIC + accelerator + limited local storage designs; less ideal if you need lots of front NVMe bays in 1U."
  },
  {
    "objectID": "ampereone_m_vs_amd_epyc.html#recommendation-a-minimal-decision-oriented-poc",
    "href": "ampereone_m_vs_amd_epyc.html#recommendation-a-minimal-decision-oriented-poc",
    "title": "AmpereOne M vs AMD EPYC 9554/9654",
    "section": "Recommendation: a minimal, decision-oriented POC",
    "text": "Recommendation: a minimal, decision-oriented POC\n\nPick one workload per ‚Äúshape‚Äù you run: (a) latency-sensitive service, (b) batch/throughput service, (c) memory-bandwidth-heavy job, (d) IO-heavy node.\nTest 2 Ampere SKUs per AMD SKU (from the shortlist) and keep memory config constant (same capacity per channel).\nCompare: throughput/socket, p99 latency, perf/W, and perf/$ (including any software/license deltas).\nFail fast on compatibility: image build, agents, monitoring, kernel/driver support, and any proprietary binaries."
  },
  {
    "objectID": "arista_switch_health_report.html",
    "href": "arista_switch_health_report.html",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "",
    "text": "This page summarizes findings from the health report file healthreport_Arista.txt."
  },
  {
    "objectID": "arista_switch_health_report.html#snapshot",
    "href": "arista_switch_health_report.html#snapshot",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "Snapshot",
    "text": "Snapshot\n\nReport timestamp: Sat Jan 31 09:41:47 UTC 2026\nModel: Arista DCS-7060CX-32S-R (HW v01.00, Mfg date 2016-04-21)\nEOS: 4.26.6M\nUptime at capture: ~20 minutes"
  },
  {
    "objectID": "arista_switch_health_report.html#overall-status",
    "href": "arista_switch_health_report.html#overall-status",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "Overall status",
    "text": "Overall status\nCooling, temperatures, and flash look healthy. The report shows PSU2 offline, but in this test setup that was expected because PSU2 was not connected to power (so the system was running without redundancy)."
  },
  {
    "objectID": "arista_switch_health_report.html#key-findings",
    "href": "arista_switch_health_report.html#key-findings",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "Key findings",
    "text": "Key findings\n\n\n\n\n\n\n\n\nArea\nStatus\nNotes\n\n\n\n\nCooling\nPass\nFans OK and stable; ambient 25¬∞C\n\n\nTemperature\nPass\nAll sensors below alert/critical thresholds\n\n\nPower supplies\nExpected (lab setup)\nPSU1 OK (~125.5W load); PSU2 shows ‚ÄúPower Loss Offline‚Äù when not connected\n\n\nModules/ASIC status\nScript false-fail\nshow module is not supported on this platform, so the report‚Äôs FAIL here is not a hardware fault by itself\n\n\nInterfaces\nN/A (bench state)\nAll ports show notconnect / Not Present (no optics/cables); Ma1 also down\n\n\nFlash\nPass\n/mnt/flash 63% used; writable"
  },
  {
    "objectID": "arista_switch_health_report.html#details",
    "href": "arista_switch_health_report.html#details",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "Details",
    "text": "Details\n\nPower (expected in this test setup)\nshow environment power indicates:\n\nPSU 1: Ok, drawing input current and supplying ~125.5W\nPSU 2: Power Loss Offline, 0.0W (expected when not connected)\n\nThis is fine for lab testing, but it removes redundancy and increases risk if PSU1 or its feed fails.\n\n\n‚ÄúModules are not reporting Active‚Äù (likely not real)\nThe report section ‚ÄúMODULE & ASIC STATUS‚Äù runs show module, but the output returns:\n\n‚ÄúUnavailable command (not supported on this hardware platform)‚Äù\n\nSo treat the report‚Äôs FAIL on that section as a limitation of the report script, not necessarily a device health issue."
  },
  {
    "objectID": "arista_switch_health_report.html#recommended-next-checks",
    "href": "arista_switch_health_report.html#recommended-next-checks",
    "title": "Arista Switch Health Report (DCS-7060CX-32S-R)",
    "section": "Recommended next checks",
    "text": "Recommended next checks\nTo restore redundancy, connect PSU2 to AC and confirm it transitions to Ok. If it still shows offline:\n\nReseat PSU2 and verify it‚Äôs the correct model for the chassis.\nSwap AC cable/outlet/circuit (or swap PSUs between slot 1 and slot 2 if you have a safe procedure).\nRe-run these commands and look for state changes:\n\nshow environment power\nshow logging | include PSU|Power|pwr|power-supply\nshow system environment cooling\nshow system environment temperature"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research",
    "section": "",
    "text": "Notes, writeups, and summaries for personal research projects."
  },
  {
    "objectID": "index.html#documents",
    "href": "index.html#documents",
    "title": "Research",
    "section": "Documents",
    "text": "Documents\n\nHDD comparison and 5-year TCO\nAmpereOne M vs AMD EPYC 9554/9654\nArista switch health report (DCS-7060CX-32S-R)\nRust primer for C++/Java developers"
  }
]